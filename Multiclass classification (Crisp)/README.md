Task

Input:

You have been given a link to a csv file containing features and labels. The final column represents
the label (as a string) and all other columns represent different hand crafted features (floating point
values).

Problem Statement:

You have to train different classifiers on this dataset and predict the labels correspoding to the
feature vectors.
Not all of the features are contrbuting for an improvement in final classification accuracy. Your task
is to try different training algorithms and features selection approaches to show which algorithm
suits the task best and which features best define the dataset.
Divide the dataset into train and validation set with 80/20 ratio and observe the validation accuracy
for different algorithms.

Output:

Share your code (preferably a jupyter notebook) with some visualization of your resultant
accuracies and a brief report over the feature selection process describing which features were most
useful and the ones which didn’t contribute much.


Approach


The task is to implement various classification algorithm on the data set. The problem is of multi-class classification with 17 output classes (LABEL). To solve this, I took following steps-

•	The first step is to analyze the data and check the output feature vector. Also we need to divide the data into training and cross-validation. It was implemented using train_test_split of sklearn library.

•	Then we use label encoders to convert our output feature vector.

•	We then implement SVM, Naïve Bayes, KNN classifier, Decision Tree classifier. Out of which, SVM gives the best accuracy.

•	We then use some feature engineering to remove some features. To do so, we calculate the Correlation Coefficient. Some of the features like DP_PREP, DP_POBJ, CG19, CG28, C_KEYW, NER_OTH are highly correlated (greater than 0.9).  So we can drop these features.

•	We then again implement our algorithms on these new feature vectors.

•	Also we can use Recursive Feature Elimination, but it turns out it is giving the same accuracy as the feature vector generated after removing highly correlated variables.

•	Then we use Principal Component Analysis to reduce the features. After reducing the number of features to 84, it shows a slight improvement in accuracy.



•	But the best output is generated by Neural Network implementation. The accuracy is far better than any other standard algorithm. The neural network implementation is in separate IPython Notebook.
